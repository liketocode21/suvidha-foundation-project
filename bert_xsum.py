# -*- coding: utf-8 -*-
"""BERT_XSUM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rGCSNCothWsVR-YRmgcNE0EuxtpsTqws
"""

# Step 1: Install Required Packages
!pip install transformers datasets rouge-score evaluate

# Step 2: Load and Preprocess the XSum Dataset
from datasets import load_dataset

# Load the XSum dataset
dataset = load_dataset("xsum")

# Step 3: Preprocess the Dataset
from transformers import BertTokenizer

# Load BERT tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Define a preprocessing function
def preprocess_data(examples):
    # Tokenize input (article) and target (summary)
    inputs = tokenizer(examples["document"], padding="max_length", truncation=True, max_length=512, return_tensors="pt")
    outputs = tokenizer(examples["summary"], padding="max_length", truncation=True, max_length=128, return_tensors="pt")
    return {
        "input_ids": inputs.input_ids.squeeze(),
        "attention_mask": inputs.attention_mask.squeeze(),
        "decoder_input_ids": outputs.input_ids.squeeze(),
        "decoder_attention_mask": outputs.attention_mask.squeeze(),
        "labels": outputs.input_ids.squeeze(),
    }

# Apply preprocessing to the dataset
tokenized_dataset = dataset.map(preprocess_data, batched=True, remove_columns=["document", "summary", "id"])

# Step 4: Load the BERT2BERT Model
from transformers import EncoderDecoderModel

# Load BERT2BERT model
model = EncoderDecoderModel.from_encoder_decoder_pretrained("bert-base-uncased", "bert-base-uncased")

# Set the model to generate the output
model.config.decoder_start_token_id = tokenizer.cls_token_id
model.config.pad_token_id = tokenizer.pad_token_id
model.config.vocab_size = model.config.encoder.vocab_size

# Step 5: Set Up Training Arguments
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results_bert2bert_xsum",
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    predict_with_generate=True
)

# Step 6: Define Compute Metrics
import evaluate
from rouge_score import rouge_scorer

# Load ROUGE metric
rouge = evaluate.load("rouge")

# Define a function to compute ROUGE scores
def compute_metrics(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions

    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    labels_ids[labels_ids == -100] = tokenizer.pad_token_id
    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)

    rouge_output = rouge.compute(predictions=pred_str, references=label_str)
    return {
        "rouge1": rouge_output["rouge1"].mid.fmeasure,
        "rouge2": rouge_output["rouge2"].mid.fmeasure,
        "rougeL": rouge_output["rougeL"].mid.fmeasure,
    }

# Step 7: Train the Model
from transformers import Trainer

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"].select(range(5000)),  # Training on a smaller subset for demo
    eval_dataset=tokenized_dataset["validation"].select(range(500)),  # Validation on a smaller subset for demo
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Start training
trainer.train()

# Step 8: Evaluate the Model
# Evaluate the model on the test set
results = trainer.evaluate(eval_dataset=tokenized_dataset["test"].select(range(500)))
print("Test set ROUGE scores:", results)