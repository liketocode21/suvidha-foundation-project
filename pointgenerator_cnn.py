# -*- coding: utf-8 -*-
"""PointGenerator_CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rGCSNCothWsVR-YRmgcNE0EuxtpsTqws
"""

import torch
import torchtext
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator

from datasets import load_dataset

# Load the dataset
dataset = load_dataset("cnn_dailymail", "3.0.0")

# Initialize tokenizer
tokenizer = get_tokenizer('spacy', language='en')

# Build vocabulary
def yield_tokens(data_iter):
    for article in data_iter:
        yield tokenizer(article['article'])

vocab = build_vocab_from_iterator(yield_tokens(dataset['train']), specials=["<unk>", "<pad>", "<bos>", "<eos>"])
vocab.set_default_index(vocab["<unk>"])

# Example of how to process one sample
def process_data(article, highlights, vocab, tokenizer, max_len=400):
    tokens = tokenizer(article)
    input_ids = [vocab["<bos>"]] + [vocab[token] for token in tokens[:max_len]] + [vocab["<eos>"]]

    tokens = tokenizer(highlights)
    target_ids = [vocab["<bos>"]] + [vocab[token] for token in tokens[:max_len]] + [vocab["<eos>"]]

    return torch.tensor(input_ids), torch.tensor(target_ids)

# Example usage
article, highlights = dataset['train'][0]['article'], dataset['train'][0]['highlights']
input_ids, target_ids = process_data(article, highlights, vocab, tokenizer)

import torch
import torch.nn as nn
import torch.nn.functional as F

class PointerGenerator(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size):
        super(PointerGenerator, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.encoder = nn.LSTM(embed_size, hidden_size, batch_first=True)
        self.decoder = nn.LSTM(embed_size, hidden_size, batch_first=True)
        self.attn = nn.Linear(hidden_size * 2, hidden_size)
        self.vocab_dist = nn.Linear(hidden_size, vocab_size)

    def forward(self, input_ids, target_ids):
        # Encode input
        embedded = self.embedding(input_ids)
        encoder_outputs, (hidden, cell) = self.encoder(embedded)

        # Decode target
        embedded = self.embedding(target_ids)
        decoder_outputs, _ = self.decoder(embedded, (hidden, cell))

        # Attention mechanism
        attn_weights = torch.bmm(decoder_outputs, encoder_outputs.transpose(1, 2))
        attn_applied = torch.bmm(attn_weights, encoder_outputs)

        # Generate the vocabulary distribution
        combined = torch.cat((decoder_outputs, attn_applied), dim=2)
        vocab_dist = F.log_softmax(self.vocab_dist(combined), dim=-1)

        return vocab_dist

# Initialize the model
vocab_size = len(vocab)
embed_size = 128
hidden_size = 256

model = PointerGenerator(vocab_size, embed_size, hidden_size)

import torch.optim as optim

# Training settings
learning_rate = 1e-3
batch_size = 16
num_epochs = 5

# Loss and optimizer
criterion = nn.NLLLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0

    for i, batch in enumerate(dataset['train']):
        article, highlights = batch['article'], batch['highlights']
        input_ids, target_ids = process_data(article, highlights, vocab, tokenizer)

        optimizer.zero_grad()
        output = model(input_ids.unsqueeze(0), target_ids.unsqueeze(0))  # Add batch dimension

        loss = criterion(output.view(-1, vocab_size), target_ids.view(-1))
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    print(f'Epoch {epoch+1}, Loss: {epoch_loss/i}')

print("Training complete!")

from rouge_score import rouge_scorer

def evaluate(model, dataset, vocab, tokenizer):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    model.eval()
    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}

    with torch.no_grad():
        for i, batch in enumerate(dataset['validation']):
            article, highlights = batch['article'], batch['highlights']
            input_ids, target_ids = process_data(article, highlights, vocab, tokenizer)

            output = model(input_ids.unsqueeze(0), target_ids.unsqueeze(0))
            output_text = ' '.join([vocab.lookup_token(idx) for idx in output.argmax(-1).squeeze().tolist()])
            target_text = ' '.join([vocab.lookup_token(idx) for idx in target_ids.tolist()])

            score = scorer.score(target_text, output_text)
            for key in scores.keys():
                scores[key].append(score[key].fmeasure)

    for key in scores.keys():
        print(f'{key}: {sum(scores[key]) / len(scores[key])}')

# Run evaluation
evaluate(model, dataset, vocab, tokenizer)